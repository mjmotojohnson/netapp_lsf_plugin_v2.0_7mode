
NetApp LSF Compute Agent Job Monitoring Script and Hot Job Detector Script
--------------------------------------------------------------------------

o) Overview

  This package contains the the LSF Compute Agent script and Hot Job
  Detector script as well as their supporting files.
  
  -Compute Agent script - Monitors NFS read and write operations performed
    by LSF jobs running on compute hosts. This script will be installed as
	an LSF ELIM to cause it to run on every compute node in the LSF cluster.
	Creates a text file per LSF job, totaling the NFS operations performed
	under a timestamp.
	
  -Hot Job Detector script - Monitors the NetApp controller performance data
    written by the ontapmon ONTAP monitoring script. When configurable
	performance thresholds are exceeded on the controllers, this script
	reads the LSF job report files generated by the NetApp Compute Agent and
	generates a report detailing the performance problems and listing the
	LSF jobs performing the most operations on the impacted controller and
	volume.
	

o) Prerequisites

  -LSF must be installed and running properly.
  
  -The NetApp ontapmon ONTAP monitoring script must be running properly.
  
  -Python version 2 (2.6) or higher must be installed on every LSF compute
   host.
  
  -SystemTap runtime must be installed on every LSF compute host.
  
  -A SystemTap module must be compiled for each kernel version running
   on an LSF compute host. This will require acquiring and installing
   the kernel-debuginfo, kernel-debuginfo-common, and kernel-devel
   packages for each different kernel version in the LSF cluster. See
   the section "Compiling the SystemTap module" below for detailed
   instructions.
   
  -A shared directory must be mounted on each compute node at the same
   path. The Compute Agent will write job report files to this path. The
   Compute Agent will also access a shared path to locate the compiled
   SystemTap module for the compute host's kernel version on a shared
   directory.
   
   
o) Components

  -elim.netapp_compute - Python Compute Agent script to run on each
    compute node via LSF ELIM configuration. Gathers LSF job NFS
	operation data.
	
  -netapp_lsf_compute_agent.conf - Configuration file for the Compute
    Agent script.
	
  -netapp_nfsmon.stp - SystemTap script to be compiled for each kernel
    version and run on each compute host by the Compute Agent script.

  -netapp_lsf_hot_job_detector.py - Python Hot Job Detector script.
    Monitors the performance XML files generated by the ontapmon ONTAP
	monitoring script. Reports discovered performance problems.
	
  -netapp_lsf_hot_job_detector.conf - Configuration file for the
    Hot Job Detector script.
  
  -netapp_lsf_hot_job_email.py - Python script which emails the contents of
    the performance problem report generated by the Hot Job Detector script.
	This script is automatically called by the Hot Job Detector script, but
	can be replaced by a different script to process the report as desired.
	
	
o) Installing the Compute Agent script as an LSF ELIM

  1. Edit $LSF_ENVDIR/lsf.shared to add a new resource for the Compute Agent:
  
        Begin Resource
          ....
          netapp_compute     Boolean 15       ()          (NetApp compute agent)       
        End Resource
		
		
     Note that the Boolean resource type is used only to create a custom
	 resource for the NetApp Compute Agent. No data is actually returned
	 to LSF by the Compute Agent.

  2. Edit $LSF_ENVDIR/lsf.cluster.<clustername> to add a new resource mapping
     for the compute agent. The mapping will be "[default]" to cause LSF to
	 run the ELIM on every compute host.
	 
        Begin ResourceMap
            ....
            netapp_compute     [default]
        End ResourceMap
	  
  3. Copy elim.netapp_compute and netapp_lsf_compute_agent.conf to $LSF_SERVERDIR.
     Make sure elim.netapp_compute has executable permissions.
	 
       $ mv elim.netapp_compute netapp_lsf_compute_agent.conf $LSF_SERVERDIR
       $ chmod +x $LSF_SERVERDIR/elim.netapp_compute
	   
  4. Compile the SystemTap modules for each different kernel version running
     on a compute host. See the section "Compiling the SystemTap module" for
	 detailed instructions.
	 
  5. Edit the configuration file for the Compute Agent script at
     $LSF_SERVERDIR/netapp_lsf_compute_agent.conf. Most values can be
	 left as default unless tweaking is required, but the following two
	 configuration options must be set:
	 
	 systemtap_modules_directory - The path to the shared directory containing
	   the compiled SystemTap module files (.ko files) for the various kernel
	   versions. This must be a shared directory mounted on the same path
	   on every comupute host.
	   
	 job_report_output_directory - The path to a shared directory where the
	   Compute Agent running on each compute host can write its LSF job
	   report files. This must be a shared directory mounted on the same
	   path on every compute host.
	   
	 Refer to the comments provided in the configuration file for details on
	 the other configuration parameters.
  
  6. Restart the LSF LIMs to cause the Compute Agent ELIM script to be run on
     every compute host.
	 
       $ lsadmin reconfig
	   
	   Checking configuration files ...
       No errors found.
       
       Restart only the master candidate hosts? [y/n] n
       Do you really want to restart LIMs on all hosts? [y/n] y

  7. A log file for the Compute Agent is created on each compute host at
     /var/log/netapp_lsf_compute_agent.log. Check this log file to look
	 for any errors.
	 

o) Compiling the SystemTap module

  A compiled SystemTap module needs to be created for each kernel version on
  which the Compute Agent will run. For each kernel version, the
  kernel-debuginfo, kernel-debuginfo-common, and kernel-devel packages need
  to be acquired and installed. Installation of these packages and compilation
  of the SystemTap module can be done on any host as the compiled module file
  will need to be copied to a shared location. The full SystemTap package
  needs to be installed on the host compiling the module, but only the
  SystemTap runtime package is required on each compute host to run the
  compiled module.
  
  
  After installing the three packages for a kernel version, copy the
  SystemTap netapp_nfsmon script to the host and run the following
  command to create the compiled module.
  
      $ stap -r <kernel_version> netapp_nfsmon.stp -m netapp_nfsmon_<kernel_version_with_underscores>
  
  Example for kernel version 2.6.32-220.el6.x86_64:
  
      $ stap -r 2.6.32-220.el6.x86_64 netapp_nfsmon.stp –m netapp_nfsmon_2_6_32_220_el6_x86_64
  
  Note that dots and dashes must be replaced with underscores in the compiled
  module name (the value following the -m parameter). This is necessary due
  to SystemTap restrictions on module names.

  Finally, copy each of the compiled SystemTap module files (.ko files) to a
  shared directory. This directory must be mounted on the same path on every
  compute host. Note the path to this directory to set in the configuration
  file for the Compute Agent.


o) Checking the LSF job report files generated by the Compute Agent

  The Compute Agent will monitor NFS operations performed on each compute
  node. When an NFS operation is performed by an LSF job process, the job
  data is written to a text file in the shared job report directory
  configured above. The file is named with the LSF cluster name and Job ID.
  The data in the text file includes the NFS device and NFS operations
  performed by the job during the current monitoring period. A timestamp
  of Unix/POSIX time is written above the operation data to indicate when
  the operations occurred.
  
  The Hot Job Detector script will read these job report files to find LSF
  jobs performing the most operations on a NetApp controller with a
  performance problem.
  
  Here's a sample job report file:
  
      NetApp LSF Compute Agent Job Report - Format:  <timestamp_in_seconds_since_epoch>, followed by a newline, followed by one or more lines in the form "<controller:/vol/vol_name>,RD,WR" where RD is the NFS Read operation count and WR is the NFS Write operation count performed by the job.
      1350403871
	  fas6280-svl13:/vol/vol_data,20411,106
      fas3070-svl21:/vol/vol_report,252,32157
      1350403901
      fas3070-svl21:/vol/vol_report,5029,48807
      1350403931
      fas3070-svl21:/vol/vol_report,6180,48592
      

o) Installing the Hot Job Detector script

  The Hot Job Detector is a standalone Python script that requires no special
  installation procedures. It can run from any host that has sufficient
  access to the directories containing the ONTAP performance XML files
  generated by the ontapmon ONTAP monitoring script and the job report text
  files generated by the Compute Agents running on the compute hosts.
  
  Before starting the script, edit the netapp_lsf_hot_job_detector.conf
  configuration file to set the paths to the configuration options.
  
    ontap_xml_data_directory - The path to the directory containing the ONTAP
	  performance XML files generated by the ontapmon ONTAP monitoring script.
	  
	lsf_job_report_directory - The path to the shared directory containing
	  the job report text files generated by the Compute Agents.
	  
  You will also need to set appropriate thresholds in the configuration file.
  The Hot Job Detector script uses measures the performance data against
  these threshold values to determine if a controller is overloaded. The
  syntax for these thresholds is the same as in the PluginPolicy and
  FilerPolicy sections of the ntapplugin.conf scheduler plugin configuration
  file. Most likely, you will want to set higher thresholds for the Hot Job
  Detector than those used in the scheduler plugin configuration so that not
  every controller too busy to take on additional jobs is considered
  overloaded, triggering an alert.
  
  When one or more performance thresholds are exceeded, the Hot Job Detector
  generates a report file detailing the performance problems and lsiting
  the LSF jobs performing the most NFS operations on the impacted controllers
  and volumes. The Hot Job Detector then executes a configurable command,
  passing the path to the generated problem report file as an additional
  argument. By default, the provided netapp_lsf_hot_job_email.py script will
  be called. This script will send the contents of the problem report file
  in an email. If you will use the netapp_lsf_hot_job_email.py script, edit
  the "toAddresses", "fromAddress", and "smtpServer" variables in the script
  before running the Hot Job Detector.
  
  After the above configuration work is completed, simply execute the script
  in the background:
  
      python netapp_lsf_hot_job_detector.py &
  
  To stop the script, use the kill command.
  
      $ ps -ef | grep python
	  root       6714  6690  0 16:27 pts/1    00:00:00 python netapp_lsf_hot_job_detector.py
	  $ kill -9 6714
	  
  
  The Hot Job Detector script logs information to
  /var/log/netapp_lsf_job_job_detector.log.
	  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  